# 测试文档 MCP server 与 client

---

运行文档位置：[scr/serverDemo/运行](./运行.md)

---

## 主要Server框架搭建方法

### 一、Python FastMCP

1. 在`Python`使用`FastMCP`构建实例化对象
2. 使用`@mcp.tool()`对定义的函数方法进行修饰

### 二、Node.js 实现


### 三、Java Spring AI 实现

---

## 主要Client测验方法

### 一、LangChain
* 使用LangChain编写客户端

### 二、Cursor
* 使用Cursor客户端配置

### 三、CLine
* 使用VSCODE中CLine配置，例如以下代码
```json
{
    "mcpServers": {
        "weather": {
            "command": "uv",
            "args": [
                "--directory",
                "/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather",
                "run",
                "weather.py"
            ]
        }
    }
}
```
* 其中使用了绝对路径来调用本端程序、

### 四、MCP Inspector
* 纯命令行格式

---
## 部分前置需求

* uv
* Python
* mcp
* FastMCP
* Starlette
* FastAPI_MCP
* ……

---
## 运行
1. 安装Inspector
```bash
npx @modelcontextprotocal/inspector
```

2. 安装依赖

```bash
uv init "mcp[cli]"
```

3. 运行程序（测试用）

```bash
mcp dev TimeServerFastMCP.py
```

4. 如果下载时候需要传递参数，则需要通过以下下载

```bash
# 传递参数 arg1 arg2
npx @modelcontextprotocol/inspector build/index.js arg1 arg2

# 传递环境变量 KEY=value  KEY2=$VALUE2
npx @modelcontextprotocol/inspector -e KEY=value -e KEY2=$VALUE2 node build/index.js

# 同时传递环境变量和参数
npx @modelcontextprotocol/inspector -e KEY=value -e KEY2=$VALUE2 node build/index.js arg1 arg2

# Use -- to separate inspector flags from serverDemo arguments
npx @modelcontextprotocol/inspector -e KEY=$VALUE -- node build/index.js -e serverDemo-flag
```

5. 调试（测试用，其余也可以使用）

```bash
npx @modelcontextprotocol/inspector uvx mcp-serverDemo-fetch
```

* 在BHC876的电脑上面的访问位置 **（以每次启动为准）**：
* `http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=a3f1b308d5d14f9d110f84896fd608ae77fee2b2836e47cc3ac11354916e0f56`

* 其他推荐的直观方便的方法：使用CherryStudio进行MCP调试。

6. 补充库 ***（暂不用）***
- fastapi-mcp 需要使用add_mcp_server函数，此处必须使用0.1.8版本，下载指令：
```bash
pip install fastmcp-api==0.1.8
```
- 而使用fastapi的方法，意味着不一定需要使用uv进行包装

---
+ 2025年10月28日23:19 HBC876（wubolieng80594)

## ***目前实现方法标注***
- 见文件：`CurrentTimeServerSSE3.py`的搭建框架和`运行.md`的执行方法。
+ 使用FastMCP和Starlette快速构建MCP框架，结合现有因素开放health检测端口，设置POST和GET两种方式访问`"/message/"`端，设置GET方法访问`/sse`本体端口，可以在页面上看到调用生成的日志。
+ 目前由于Nexent网站只支持sse框架，在本地部署的基础上（localhost，即`127.0.0.1:post`，使用实际但不相邻的端口进行部署）进行SSE通信，所以目前依赖本方法集成。
+ Python的FastMCP使得SSE通信方便了许多。
+ 此外，需要注意的是，有关的模型有必要关注其调用工具的特性，是否支持`MCP协议`和能够调用MCP工具。其中反例为**使用Cherry Studio集成以后使用Qwen2.5-7B-Instructor试图调用MCP工具但完全失败**
+ 正例为使用智谱平台开放测试的**GLM-4.5-Flash**进行MCP工具调用，在部署MCP服务器的情况下不需要特别训练或配置就可以达到理想的效果。**注意**如果发现模型试图在已成功接入MCP服务器的情况下（不论是以**STDIO**还是**SSE**还是**HTTP Stream**方式）使用`<Tool_Call>...</Tool_Call>`语句进行调用工具，说明有可能其不支持此方式（存在**MCP**对接方式或成功内置**MCP Client**），建议更换其他模型，而不是继续训练。
+ 注：不支持MCP工具调用或需要训练MCP工具调用指令的模型（目前已测试，在持续更新）：
```
(Ollama localhost) qwen2:7b
(Ollama localhost) qwen2.5:7b
(Siliconflow api) Qwen2.5-7B-Instruct
(Siliconflow api) Qwen3-8B
(Siliconflow api) Qwen3-A3B (备注：此类Qwen3的基模，基本上必须进行底层配置和训练才能调工具)
(siliconflow api) deepseek-r1 (思考模型、思考模式可能对这个不友好，推理模型比较有效)
(siliconflow api) deepseek-v3 (比较奇怪，但思考太多，容易偏执)
```
* 注：目前调用MCP工具测试有效的模型（目前在测试，在持续更新）：
```
(http://27.159.93.56:8198/v1/) GLM-4.5-Flash (企业服务器，不建议再次使用以防侵害后台数据)
(open-bigmodel) GLM-4.5-Flash (确认支持MCP)
(open-bigmodel) GLM-4.5-* (本系列)

```

* 注：目前在本机（BHC876所属）的Cherry Studio与Cursor上连接的模型端点平台
```
OLLAMA 奧拉瑪-本機部署（http://localhost:11434)
siliconflow 硅基流動-API接口
open-bigmodel 智譜開放大模型-API接口
infini-ai 無問芯穹-API接口

```